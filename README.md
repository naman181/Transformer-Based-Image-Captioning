# Transformer-Based-Image-Captioning

# INTRODUCTION
Deep Learning techniques have revolutionized the computer vision domain in enabling machines to gain meaningful insights from the visual data. The important advancement is seen in the field through the emergence of mage captioning, which produces a natural language description equivalent to visual material [1]. Image captioning, by merging the principles of computer vision and NLP, works as an important connecting bridge between visual perception and verbal expression [2]. The evolution from captioning to full and meaningful stories in an important breakthrough in machine-generated content. The generation process, alongside contextual reasoning and narrational structuring actually needs the comprehension of visual components themselves [3]. It will be possible to provide better, more insightful, and more appropriate image analysis related to narrative construction for educational needs or application in digital media and assistive technologies broadly [4]. This report discusses the design of a comprehensive framework combining image captioning and story generation using advanced architectures in deep learning.


# Project Architecture
![image](https://github.com/user-attachments/assets/c78a65ba-0663-453b-97b9-effee780db89)


# IMPLEMENTATION

# 1. CNN Encoder
The CNN Encoder captures high-level features of input images. Such features are used as embeddings for caption generation (CNN_Encoder.py).

# 2. Transformer Encoder
The Transformer Encoder takes the feature maps generated by the CNN Encoder. It fine-tunes these embeddings using self-attention mechanisms, which capture long-range dependencies and relationships between features (Transformer_Encoder.py).

# 3. Transformer Decoder
The Transformer Decoder produces captions one word at a time given the encoded image features. It utilizes the embeddings for the previously generated words, as well as the features from the image, in order to predict the next word (Transformer_Decoder.py).

# 4. Image Captioning Model:
The ImageCaptioningModel is a custom TensorFlow Keras model designed for image captioning: it combines a CNN-like InceptionV3 for the extraction of features from an image and a Transformer architecture set up in an encoder-decoder fashion to generate captions conditioned on those features. During both training and evaluation, it computes loss and accuracy for ensuring that captions are indeed generated word by word conditioned on both the image features and on the previous words in the caption (Image_Captioning.py).


# Results

Example Image 1:
a)	Predicted Caption: “A black dog is running in a field.”

b)	Ground Truth Caption: “A black dog running in the grass.”

![image](https://github.com/user-attachments/assets/28ba1b11-4490-476f-96ec-b2506d40b3c6)


Example Image 2:
a)	Predicted Caption: “A man in a black shirt and a woman in a black shirt.”

b)	Ground Truth Caption: “A man standing in front of a birdcage.”

![image](https://github.com/user-attachments/assets/3204d686-83d6-4bee-9d7b-8b1e980f9fdb)


# TABULAR REPRESENTATION OF RESULT

![image](https://github.com/user-attachments/assets/5e15ed52-9a19-491d-9820-a86aea1c04d7)


# Conclusion
This project is designed to develop an image captioning model using the Transformer architecture, incorporating InceptionV3 pre-trained CNN for feature extraction and Transformer layers for caption generation. The model was trained on the Flickr8K dataset and was able to generate captions for unseen images, which proves the viability of the integration of computer vision with natural language processing. The encoder-decoder architecture was able to map image features into meaningful textual descriptions.
